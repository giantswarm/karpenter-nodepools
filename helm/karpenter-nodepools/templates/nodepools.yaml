{{- range $name, $value := .Values.nodePools }}
apiVersion: karpenter.sh/v1
kind: NodePool
metadata:
  labels:
    {{- include "labels.common" $ | nindent 4 }}
  name: {{ $.Values.clusterName }}-{{ $name }}
spec:
  # Template section that describes how to template out NodeClaim resources that Karpenter will provision
  # Karpenter will consider this template to be the minimum requirements needed to provision a Node using this NodePool
  # It will overlay this NodePool with Pods that need to schedule to further constrain the NodeClaims
  # Karpenter will provision to launch new Nodes for the cluster
  template:
    metadata:
      # Labels are arbitrary key-values that are applied to all nodes
      labels:
        giantswarm.io/machine-pool: {{ $.Values.clusterName }}-{{ $name }}
        {{- with $value.customNodeLabels }}
        {{- range . }}
        {{- $parts := splitList "=" . }}
        {{- if eq (len $parts) 2 }}
        {{ index $parts 0 }}: {{ index $parts 1 | quote }}
        {{- end }}
        {{- end }}
        {{- end }}
    spec:
      # The amount of time a Node can live on the cluster before being removed.
      # Avoiding long-running Nodes helps to reduce security vulnerabilities as well as to reduce the chance of issues that can plague Nodes with long uptimes such as file fragmentation or memory leaks from system processes.
      # You can choose to disable expiration entirely by setting the string value 'Never' here.
      # Note: changing this value in the nodepool will drift the nodeclaims.
      expireAfter: {{ $value.expireAfter }}
      {{- if $value.customNodeTaints }}
      # Provisioned nodes will have these taints.
      taints:
      {{- range $value.customNodeTaints }}
        - key: {{ .key }}
          effect: {{ .effect }}
          value: {{ .value | quote }}
      {{- end }}
      {{- end }}
      # Provisioned nodes will have these taints, but pods do not need to tolerate these taints to be provisioned by this
      # NodePool. These taints are expected to be temporary and some other entity (e.g. a DaemonSet) is responsible for
      # removing the taint after it has finished initializing the node.
      startupTaints:
        - effect: NoExecute
          key: node.cilium.io/agent-not-ready
          value: "true"
        - effect: NoExecute
          key: node.cluster.x-k8s.io/uninitialized
          value: "true"
      # References the Cloud Provider's NodeClass resource, see your cloud provider specific documentation
      nodeClassRef:
        group: karpenter.k8s.aws  # Updated since only a single version will be served
        kind: EC2NodeClass
        name: {{ $.Values.clusterName }}-{{ $name }}

      # Requirements that constrain the parameters of provisioned nodes.
      # These requirements are combined with pod.spec.topologySpreadConstraints, pod.spec.affinity.nodeAffinity, pod.spec.affinity.podAffinity, and pod.spec.nodeSelector rules.
      # Operators { In, NotIn, Exists, DoesNotExist, Gt, and Lt } are supported.
      # https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/#operators
      requirements:
        {{- range $value.requirements }}
        - key: {{ .key }}
          operator: {{ .operator }}
          values:
            {{- range .values }}
            - "{{ . }}"
            {{- end }}
        {{- end }}

  # Resource limits constrain the total size of the pool.
  # Limits prevent Karpenter from creating new instances once the limit is exceeded.
  limits:
    {{- if $value.limits }}
    cpu: {{ $value.limits.cpu | default "1000" }}
    memory: {{ $value.limits.memory | default "1000Gi" }}
    {{- else }}
    cpu: "1000"
    memory: "1000Gi"
    {{- end }}
{{- end }}
